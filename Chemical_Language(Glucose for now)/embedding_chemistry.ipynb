{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d906755",
   "metadata": {},
   "source": [
    "# Chemical Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "enamine_fingerprints_df = pd.read_csv('enamine_fingerprints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import seaborn as sns\n",
    "\n",
    "fingerprints = np.array(enamine_fingerprints_df['Morgan_fingerprint'].tolist())\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(fingerprints)\n",
    "\n",
    "enamine_fingerprints_df['UMAP1'] = embedding[:, 0]\n",
    "enamine_fingerprints_df['UMAP2'] = embedding[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.jointplot(data=enamine_fingerprints_df, x='UMAP1', y='UMAP2', kind=\"hex\", color=\"#4CB391\")\n",
    "\n",
    "plt.title('UMAP projection of Morgan Fingerprints')\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e04bff",
   "metadata": {},
   "source": [
    "Okay let's take a swing at Enamine Building Blocks, I found a list of 500K of them. that should be a good start.\n",
    "Can we create their morgan fingerprints and cluster them?\n",
    "\n",
    "Taken from https://zinc12.docking.org/db/byvendor/enaminebb/enaminebb.in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f831fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enamine_df = pd.read_csv('enaminebb.in.txt', delimiter=' ', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78efa017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats start index?  236000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 188/286 [07:21<04:32,  2.78s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def get_single_molecule_embedding(smiles, radius=5, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "        return list(fp)\n",
    "    return None\n",
    "\n",
    "def process_and_append(\n",
    "    df, start_idx=0, chunk_size=1000, output_file=\"final_processed_data.csv\"\n",
    "):\n",
    "    for start in tqdm(range(start_idx, len(df), chunk_size)):\n",
    "        end = start + chunk_size\n",
    "        chunk = df.iloc[start:end].copy()  # Using copy to safely modify the data\n",
    "        chunk[\"Morgan_fingerprint\"] = chunk[\"SMILES\"].apply(\n",
    "            get_single_molecule_embedding\n",
    "        )\n",
    "        mode = \"a\" if start > 0 else \"w\"  # 'w' for write on first chunk, 'a' for append on subsequent chunks\n",
    "        header = start == 0  # Write header only for the first chunk\n",
    "        chunk.to_csv(output_file, mode=mode, header=header, index=False)\n",
    "\n",
    "def check_progress(output_file):\n",
    "    \"\"\"Check if the output file already exists and determine the last processed index\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "            df = pd.read_csv(output_file)\n",
    "            last_index_processed = df.index[-1]\n",
    "            return last_index_processed + 1\n",
    "        else:\n",
    "            return 0\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "# Check for existing progress and determine where to resume\n",
    "output_file = \"enamine_fingerprints.csv\"\n",
    "start_index = check_progress(output_file)\n",
    "print('whats start index? ', start_index)\n",
    "\n",
    "# Process DataFrame from the last checkpoint\n",
    "process_and_append(\n",
    "    enamine_df, start_idx=start_index, chunk_size=1000, output_file=output_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa049d",
   "metadata": {},
   "source": [
    "# SCRAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ecfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('zinc_stock.hdf5', 'r+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b550364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: table\n",
      "Dataset: table/axis0 - Shape: (1,) - Data Type: |S9\n",
      "Dataset: table/axis1 - Shape: (17422831,) - Data Type: int64\n",
      "Dataset: table/block0_items - Shape: (1,) - Data Type: |S9\n",
      "Dataset: table/block0_values - Shape: (1,) - Data Type: object\n",
      "\n",
      "No SMILES strings found in the datasets.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def print_h5_structure(name, obj):\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        print(f\"Group: {name}\")\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        print(f\"Dataset: {name} - Shape: {obj.shape} - Data Type: {obj.dtype}\")\n",
    "\n",
    "def search_smiles_in_h5(file):\n",
    "    smiles_datasets = []\n",
    "    file.visititems(print_h5_structure)\n",
    "    \n",
    "    def check_for_smiles(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            data = obj[:]\n",
    "            if 'smiles' in name.lower() or 'smiles' in str(data).lower():\n",
    "                smiles_datasets.append(name)\n",
    "    \n",
    "    file.visititems(check_for_smiles)\n",
    "    \n",
    "    return smiles_datasets\n",
    "\n",
    "# Open the HDF5 file\n",
    "file_path = 'zinc_stock.hdf5'\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    smiles_datasets = search_smiles_in_h5(f)\n",
    "    if smiles_datasets:\n",
    "        print(\"\\nDatasets likely containing SMILES strings:\")\n",
    "        for dataset in smiles_datasets:\n",
    "            print(dataset)\n",
    "    else:\n",
    "        print(\"\\nNo SMILES strings found in the datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21020ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def print_dataset_contents(file, dataset_name):\n",
    "    data = file[dataset_name][:]\n",
    "    print(f\"Contents of {dataset_name}:\")\n",
    "    print(data)\n",
    "    print()\n",
    "\n",
    "file_path = 'zinc_stock.hdf5'\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    print_dataset_contents(f, 'table/axis0')\n",
    "    print_dataset_contents(f, 'table/axis1')\n",
    "    print_dataset_contents(f, 'table/block0_items')\n",
    "    print_dataset_contents(f, 'table/block0_values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439edc14",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "the STRING opcode argument must be quoted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m enamine_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chemical_language/lib/python3.9/site-packages/pandas/io/pickle.py:208\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: the STRING opcode argument must be quoted"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_file = \"processed_data.pkl\"\n",
    "enamine_df = pd.read_pickle(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9d893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enamine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7588ff",
   "metadata": {},
   "source": [
    "We will use AIZynthFinder for the retrosynthesis planning\n",
    "\n",
    "https://github.com/MolecularAI/aizynthfinder\n",
    "\n",
    "https://molecularai.github.io/aizynthfinder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_model.onnx to uspto\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_templates.csv.gz to uspto\n",
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_model.onnx to ringbreaker\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_templates.csv.gz to ringbreaker\n",
      "Loading filter policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_filter_model.onnx to uspto\n",
      "Loading stock from InMemoryInchiKeyQuery to zinc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb262d1ed9494311ba93173220fb5427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='SMILES')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab558fd253884be3aa876bc3c3744c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid silver', height='180px', width='50%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c4b170e88545109ab8fe4287162f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Label(value='Stocks'), Checkbox(value=True, description='zinc', style=Descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0375baab1e48b6b6549633bf97f84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Run Search', style=ButtonStyle()), Button(description='Extend Search', styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df33e54c6074f90a3d5c3b22da26ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid silver', height='320px', overflow='auto', width='99%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e1446aaae14cada371620d6699be4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Show Reactions', style=ButtonStyle()), Dropdown(description='Routes: ', opt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8d0420ce0245a3a06720a5022d8d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid silver', width='99%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aizynthfinder.interfaces import AiZynthApp\n",
    "app = AiZynthApp(\"config.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_model.onnx to uspto\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_templates.csv.gz to uspto\n",
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_model.onnx to ringbreaker\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_templates.csv.gz to ringbreaker\n",
      "Loading filter policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_filter_model.onnx to uspto\n",
      "Loading stock from InMemoryInchiKeyQuery to zinc\n",
      "Selected as stock: zinc\n",
      "Compounds in stock: 17422831\n",
      "Selected as expansion policy: uspto\n",
      "Selected as filter policy: uspto\n",
      "Trees saved to trees.json                                                       \n",
      "Scores for best routes: 0.9976, 0.9940, 0.9940, 0.9940, 0.9940\n",
      "target: COc1cccc(OC(=O)/C=C/c2cc(OC)c(OC)c(OC)c2)c1\n",
      "search time: 110.15064024925232\n",
      "first solution time: 1.397794246673584\n",
      "first solution iteration: 1\n",
      "number of nodes: 282\n",
      "max transforms: 6\n",
      "max children: 19\n",
      "number of routes: 101\n",
      "number of solved routes: 34\n",
      "top score: 0.9976287063411217\n",
      "is solved: True\n",
      "number of steps: 1\n",
      "number of precursors: 2\n",
      "number of precursors in stock: 2\n",
      "precursors in stock: COc1cccc(OC(C)=O)c1, COc1cc(C=O)cc(OC)c1OC\n",
      "precursors not in stock: \n",
      "precursors availability: zinc;zinc\n",
      "policy used counts: {'uspto': 281}\n",
      "profiling: {'expansion_calls': 232, 'reactants_generations': 3403, 'iterations': 100}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!aizynthcli --config config.yml --smiles \"COc1cccc(OC(=O)/C=C/c2cc(OC)c(OC)c(OC)c2)c1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming enamine_df is your DataFrame and it has a column 'SMILES'\n",
    "# Applying the fingerprint function to each SMILES entry\n",
    "tqdm.pandas(desc=\"Generating fingerprints\")\n",
    "enamine_df[\"Morgan_fingerprint\"] = enamine_df[\"SMILES\"].progress_apply(\n",
    "    get_single_molecule_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a8c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 6, 4, 39, 44, 97, 38]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tqdm as tqdm\n",
    "\n",
    "# # TODO: add a column to enamine_df that is called \"Morgan_fingerprint\"\n",
    "# for idx, row in tqdm(enamine_df.iterrows(), total=len(enamine_df)):\n",
    "#     enamine_df.iloc[idx][\"Morgan Fingerprint\"] = get_molecule_embeddings(\n",
    "#         [row[\"SMILES\"]]\n",
    "#     )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f8248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # # Example usage\n",
    "# smiles_list = [\n",
    "#     \"C(C(=O)O)N\",\n",
    "#     \"CCO\",\n",
    "#     \"C1=CC=CC=C1\",\n",
    "#     \"COc1cc2ccccc2cc1C(=O)O\",\n",
    "#     \"c1ccc2c(c1)C(=O)c3cccc(c3C2=O)C(=O)O\",\n",
    "#     \"c1cc(cc(c1)Oc2cccc(c2)C(F)(F)F)C3N(C(=O)CS3)CCN4CCCCC4\",\n",
    "#     \"COC(=O)c1cccc(c1S(=O)(=O)F)N\"\n",
    "# ]  # Glycine, Ethanol, Benzene\n",
    "# fingerprints = get_molecule_embeddings(smiles_list)\n",
    "# # print(fingerprints)\n",
    "# # print(fingerprints[0].sum())\n",
    "# # print(fingerprints[1].sum())\n",
    "# [fp.sum() for fp in fingerprints]\n",
    "\n",
    "import numpy as np\n",
    "# print(np.array(fingerprints[2]).where(1))\n",
    "numpy_arr = np.array(fingerprints[2])\n",
    "[idx for idx, val  in enumerate(numpy_arr) if val > 0]\n",
    "fingerprints[2][389]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d73f40",
   "metadata": {},
   "source": [
    "## Taking a step back... nouns can be just described by their constitutive atoms? But complex nouns then get out of grasp... like how would you chemically describe a human?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open('most_common_words.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Word', 'Frequency'])  # Writing header\n",
    "    writer.writerows(most_common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f20f04",
   "metadata": {},
   "source": [
    "# PubChem to the rescue\n",
    "\n",
    "FTP to download all of the compounds in pubchem as a SMILES -> ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound/Extras/CID-SMILES.gz\n",
    "https://chemistry.stackexchange.com/questions/122109/how-to-get-the-smiles-of-all-compounds-on-pubchem \n",
    "\n",
    "Hopefully it's only a couple of gigabytes\n",
    "\n",
    "Then to get the 1000 most common molecules is a bit difficult. but perhpas something I can do is just cheat:\n",
    "* Either take the 1000 cheapest molecules\n",
    "* Or take the 1000 molecules with the shortest SMILE strings (That ensures I get all of the elements, H2O, methane, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671e0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f4a440a",
   "metadata": {},
   "source": [
    "Maybe not necessary, but still may help is to bring them both down to the same dimension size with PCA\n",
    "This step can help in normalizing the scale and variance of the datasets.\n",
    "Especially if I bring them both lower... maybe 100?\n",
    "\n",
    "* Canonical Correlation Analysis (CCA): If you seek more sophisticated statistical techniques, CCA can be used to find the relationships between two sets of variables. This might give insight`s into how molecular structures correlate with semantic meanings of words.\n",
    "* Procrustes Analysis: This method is used to find the optimal alignment of two datasets by minimizing the distance between them after allowing for translation, scaling, and rotation. It's more complex but can effectively align two spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b87d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2830541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "\n",
    "# Assume matrix1 and matrix2 are your datasets (numpy arrays)\n",
    "# They should be of the same dimensions\n",
    "\n",
    "# Apply Procrustes Analysis\n",
    "mtx1_transformed, mtx2_transformed, disparity = procrustes(matrix1, matrix2)\n",
    "\n",
    "# mtx1_transformed and mtx2_transformed are the transformed matrices\n",
    "# 'disparity' measures the sum of the squared differences (lower is better fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "\n",
    "# Assume X and Y are your datasets (numpy arrays) with the same number of samples\n",
    "# X could be word embeddings, and Y could be molecular fingerprints, both already preprocessed\n",
    "\n",
    "# Instantiate and fit the CCA model\n",
    "cca = CCA(n_components=2)  # You can adjust 'n_components' based on your specific needs\n",
    "cca.fit(X, Y)\n",
    "\n",
    "# Transform the datasets\n",
    "X_c, Y_c = cca.transform(X, Y)\n",
    "\n",
    "# X_c and Y_c are the projections of X and Y onto the learned canonical variables\n",
    "# You can now analyze the correlations or use these projections for further analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
