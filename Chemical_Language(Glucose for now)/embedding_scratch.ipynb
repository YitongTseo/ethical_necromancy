{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530471b4",
   "metadata": {},
   "source": [
    "# Let's take a look at the General Service List (list of common english words) ... 2000 words or so\n",
    "\n",
    "https://www.eapfoundation.com/vocab/general/gsl/\n",
    "\n",
    "# Corpus of Contemporary American English (COCA) <-- sorted by frequency \n",
    "https://www.wordfrequency.info/ \n",
    "UGH i have to purchase this? booo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cd6bc",
   "metadata": {},
   "source": [
    "# Game plan...\n",
    "* I want certain reactive groups to correlate with whether things are nouns or verbs.\n",
    "    * I think that means I have to somehow separate out reactive vs. unreactive chemicals first.\n",
    "    * Similarly separate out verbs and nouns in english (that should have a function already)\n",
    "* Then I want to maybe match the most common words in each category using COCA for english and some other sort of frequency of most common chemicals...\n",
    "* After frequency, let's rely on embeddings to get everything else closest to the nearest neighbor anchor point. This way we can be sure things are at least somewhat aligned.\n",
    "    * alternatively we don't even start with the anchors but jump straight into the embeddings? Need to talk to David to get his help here with alignment.\n",
    "\n",
    "\n",
    "* Subjunctive / conditional / other fun tenses could be inert gases that I add to the reaction like Helium could be a good example of conditional.\n",
    "* Electro chemistry would be crazy... maybe that can be for a tense that is crazy... again I want to say conditional.\n",
    "\n",
    "* Past, present, future will be decided by which side of a reaction is given. if it's in equilibrium then it's present.\n",
    "\n",
    "I guess stoichometry is important here :^P\n",
    "\n",
    "* One thing I wish would be that we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14358cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fb37236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag descriptions based on the Penn Treebank tag set\n",
    "pos_tag_descriptions = {\n",
    "    \"CC\": \"Coordinating conjunction\",\n",
    "    \"CD\": \"Cardinal number\",\n",
    "    \"DT\": \"Determiner\",\n",
    "    \"EX\": \"Existential there\",\n",
    "    \"FW\": \"Foreign word\",\n",
    "    \"IN\": \"Preposition or subordinating conjunction\",\n",
    "    \"JJ\": \"Adjective\",\n",
    "    \"JJR\": \"Adjective\",\n",
    "    \"JJS\": \"Adjective\",\n",
    "    \"LS\": \"List item marker\",\n",
    "    \"MD\": \"Modal\",\n",
    "    \"NN\": \"Noun\",\n",
    "    \"NNS\": \"Noun\",\n",
    "    \"NNP\": \"Noun\", # Proper Noun\n",
    "    \"NNPS\": \"Noun\",\n",
    "    \"PDT\": \"Predeterminer\",\n",
    "    \"POS\": \"Possessive ending\",\n",
    "    \"PRP\": \"Noun\", # Personal pronoun\n",
    "    \"PRP$\": \"Noun\", # Possessive pronoun\n",
    "    \"RB\": \"Adverb\",\n",
    "    \"RBR\": \"Adverb\",\n",
    "    \"RBS\": \"Adverb\",\n",
    "    \"RP\": \"Particle\",\n",
    "    \"SYM\": \"Symbol\",\n",
    "    \"TO\": \"to\",\n",
    "    \"UH\": \"Interjection\",\n",
    "    \"VB\": \"Verb\",\n",
    "    \"VBD\": \"Verb\",\n",
    "    \"VBG\": \"Verb\",\n",
    "    \"VBN\": \"Verb\",\n",
    "    \"VBP\": \"Verb\",\n",
    "    \"VBZ\": \"Verb\",\n",
    "    \"WDT\": \"Wh-determiner\",\n",
    "    \"WP\": \"Wh-pronoun\",\n",
    "    \"WP$\": \"Possessive wh-pronoun\",\n",
    "    \"WRB\": \"Wh-adverb\",\n",
    "    \"``\": \"Punctuation\",\n",
    "    \"''\": \"Punctuation\",\n",
    "    \",\": \"Punctuation\",\n",
    "    \".\": \"Punctuation\",\n",
    "    \":\": \"Punctuation\",\n",
    "    \"(\": \"Punctuation\",\n",
    "    \")\": \"Punctuation\",\n",
    "    \"$\": \"Punctuation\",\n",
    "    \"#\": \"Punctuation\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "778239f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to determine the lemma based on POS tag\n",
    "def get_lemma(word, tag):\n",
    "    # Exclude words that are entirely numeric or symbols\n",
    "    if re.fullmatch(r'[\\d$.,]+', word):\n",
    "        return None  # Return None to indicate exclusion\n",
    "    \n",
    "    # Handle possessive cases\n",
    "    if word.endswith(\"'s\") or word.endswith(\"’s\"):\n",
    "        word = word[:-2]  # Remove 's\n",
    "    elif word.endswith(\"'\") or word.endswith(\"’\"):\n",
    "        word = word[:-1]  # Remove trailing apostrophe\n",
    "\n",
    "    if tag.startswith('NN'):\n",
    "        return lemmatizer.lemmatize(word, pos='n')  # for nouns\n",
    "    elif tag.startswith('VB'):\n",
    "        return lemmatizer.lemmatize(word, pos='v')  # for verbs\n",
    "    elif tag.startswith('JJ'):\n",
    "        return lemmatizer.lemmatize(word, pos='a')  # for adjectives\n",
    "    elif tag.startswith('RB'):\n",
    "        return lemmatizer.lemmatize(word, pos='r')  # for adverbs\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "\n",
    "words = [word.lower() for word in brown.words()]\n",
    "tagged_words = pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4051a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_lemmas = [\n",
    "    (get_lemma(word, pos), pos_tag_descriptions[pos]) for (word, pos) in tagged_words\n",
    "]\n",
    "total_freq_dist = FreqDist(tagged_lemmas)\n",
    "sorted_words_by_POS = {}\n",
    "for (lemma, pos), count in total_freq_dist.items():\n",
    "    sorted_words_by_POS[pos] = sorted_words_by_POS.get(pos, []) + [(lemma, count)]\n",
    "\n",
    "for pos, word_list in sorted_words_by_POS.items():\n",
    "    sorted_words_by_POS[pos] = sorted(word_list, key=lambda item: item[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a77be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24469"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_words_by_POS['Noun'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e77df5",
   "metadata": {},
   "source": [
    "### Now let's get their embeddings and roughly cluster the top 1000 or so words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_word_embeddings(words):\n",
    "    inputs = tokenizer(words, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Word:\n",
    "    pos: str\n",
    "    word: str\n",
    "    count: int\n",
    "\n",
    "    def get_embedding(self) -> float:\n",
    "        return get_word_embeddings([self.word])[0]\n",
    "\n",
    "\n",
    "# sorted_words_embeddings_by_POS = {}\n",
    "\n",
    "# for pos, word_list in sorted_words_by_POS.items():\n",
    "#     sorted_words_by_POS[pos] = sorted(word_list, key=lambda item: item[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d906755",
   "metadata": {},
   "source": [
    "# The rest is easy enough... what about getting the top molecules now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e04bff",
   "metadata": {},
   "source": [
    "Okay let's take a swing at Enamine Building Blocks, I found a list of 500K of them. that should be a good start.\n",
    "Can we create their morgan fingerprints and cluster them?\n",
    "\n",
    "Taken from https://zinc12.docking.org/db/byvendor/enaminebb/enaminebb.in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f831fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enamine_df = pd.read_csv('enaminebb.in.txt', delimiter=' ', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "78efa017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/521 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521/521 [21:59<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_single_molecule_embedding(smiles, radius=5, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "        return list(fp)\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_and_append(\n",
    "    df, start_idx=0, chunk_size=1000, output_file=\"final_processed_data.csv\"\n",
    "):\n",
    "    for start in tqdm(range(start_idx, len(df), chunk_size)):\n",
    "        end = start + chunk_size\n",
    "        chunk = df.iloc[start:end].copy()  # Using copy to safely modify the data\n",
    "        chunk[\"Morgan_fingerprint\"] = chunk[\"SMILES\"].apply(\n",
    "            get_single_molecule_embedding\n",
    "        )\n",
    "        mode = (\n",
    "            \"a\" if start > 0 else \"w\"\n",
    "        )  # 'w' for write on first chunk, 'a' for append on subsequent chunks\n",
    "        header = True if start == 0 else False  # Write header only for the first chunk\n",
    "        chunk.to_csv(output_file, mode=mode, header=header, index=False)\n",
    "\n",
    "\n",
    "def check_progress(output_file):\n",
    "    \"\"\"Check if the output file already exists and determine the last processed index\"\"\"\n",
    "    try:\n",
    "        last_part = pd.read_pickle(output_file)\n",
    "        last_index_processed = last_part.index[-1]\n",
    "        return last_index_processed + 1\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Check for existing progress and determine where to resume\n",
    "output_file = \"final_processed_data.pkl\"\n",
    "start_index = check_progress(output_file)\n",
    "\n",
    "# Process DataFrame from the last checkpoint\n",
    "process_and_append(\n",
    "    enamine_df, start_idx=start_index, chunk_size=1000, output_file=output_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming enamine_df is your DataFrame and it has a column 'SMILES'\n",
    "# Applying the fingerprint function to each SMILES entry\n",
    "tqdm.pandas(desc=\"Generating fingerprints\")\n",
    "enamine_df[\"Morgan_fingerprint\"] = enamine_df[\"SMILES\"].progress_apply(\n",
    "    get_single_molecule_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "264a8c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 6, 4, 39, 44, 97, 38]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tqdm as tqdm\n",
    "\n",
    "# # TODO: add a column to enamine_df that is called \"Morgan_fingerprint\"\n",
    "# for idx, row in tqdm(enamine_df.iterrows(), total=len(enamine_df)):\n",
    "#     enamine_df.iloc[idx][\"Morgan Fingerprint\"] = get_molecule_embeddings(\n",
    "#         [row[\"SMILES\"]]\n",
    "#     )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # # Example usage\n",
    "# smiles_list = [\n",
    "#     \"C(C(=O)O)N\",\n",
    "#     \"CCO\",\n",
    "#     \"C1=CC=CC=C1\",\n",
    "#     \"COc1cc2ccccc2cc1C(=O)O\",\n",
    "#     \"c1ccc2c(c1)C(=O)c3cccc(c3C2=O)C(=O)O\",\n",
    "#     \"c1cc(cc(c1)Oc2cccc(c2)C(F)(F)F)C3N(C(=O)CS3)CCN4CCCCC4\",\n",
    "#     \"COC(=O)c1cccc(c1S(=O)(=O)F)N\"\n",
    "# ]  # Glycine, Ethanol, Benzene\n",
    "# fingerprints = get_molecule_embeddings(smiles_list)\n",
    "# # print(fingerprints)\n",
    "# # print(fingerprints[0].sum())\n",
    "# # print(fingerprints[1].sum())\n",
    "# [fp.sum() for fp in fingerprints]\n",
    "\n",
    "import numpy as np\n",
    "# print(np.array(fingerprints[2]).where(1))\n",
    "numpy_arr = np.array(fingerprints[2])\n",
    "[idx for idx, val  in enumerate(numpy_arr) if val > 0]\n",
    "fingerprints[2][389]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d73f40",
   "metadata": {},
   "source": [
    "## Taking a step back... nouns can be just described by their constitutive atoms? But complex nouns then get out of grasp... like how would you chemically describe a human?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open('most_common_words.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Word', 'Frequency'])  # Writing header\n",
    "    writer.writerows(most_common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f20f04",
   "metadata": {},
   "source": [
    "# PubChem to the rescue\n",
    "\n",
    "FTP to download all of the compounds in pubchem as a SMILES -> ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound/Extras/CID-SMILES.gz\n",
    "https://chemistry.stackexchange.com/questions/122109/how-to-get-the-smiles-of-all-compounds-on-pubchem \n",
    "\n",
    "Hopefully it's only a couple of gigabytes\n",
    "\n",
    "Then to get the 1000 most common molecules is a bit difficult. but perhpas something I can do is just cheat:\n",
    "* Either take the 1000 cheapest molecules\n",
    "* Or take the 1000 molecules with the shortest SMILE strings (That ensures I get all of the elements, H2O, methane, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671e0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f4a440a",
   "metadata": {},
   "source": [
    "Maybe not necessary, but still may help is to bring them both down to the same dimension size with PCA\n",
    "This step can help in normalizing the scale and variance of the datasets.\n",
    "Especially if I bring them both lower... maybe 100?\n",
    "\n",
    "* Canonical Correlation Analysis (CCA): If you seek more sophisticated statistical techniques, CCA can be used to find the relationships between two sets of variables. This might give insight`s into how molecular structures correlate with semantic meanings of words.\n",
    "* Procrustes Analysis: This method is used to find the optimal alignment of two datasets by minimizing the distance between them after allowing for translation, scaling, and rotation. It's more complex but can effectively align two spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b87d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2830541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "\n",
    "# Assume matrix1 and matrix2 are your datasets (numpy arrays)\n",
    "# They should be of the same dimensions\n",
    "\n",
    "# Apply Procrustes Analysis\n",
    "mtx1_transformed, mtx2_transformed, disparity = procrustes(matrix1, matrix2)\n",
    "\n",
    "# mtx1_transformed and mtx2_transformed are the transformed matrices\n",
    "# 'disparity' measures the sum of the squared differences (lower is better fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "\n",
    "# Assume X and Y are your datasets (numpy arrays) with the same number of samples\n",
    "# X could be word embeddings, and Y could be molecular fingerprints, both already preprocessed\n",
    "\n",
    "# Instantiate and fit the CCA model\n",
    "cca = CCA(n_components=2)  # You can adjust 'n_components' based on your specific needs\n",
    "cca.fit(X, Y)\n",
    "\n",
    "# Transform the datasets\n",
    "X_c, Y_c = cca.transform(X, Y)\n",
    "\n",
    "# X_c and Y_c are the projections of X and Y onto the learned canonical variables\n",
    "# You can now analyze the correlations or use these projections for further analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
