{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530471b4",
   "metadata": {},
   "source": [
    "# Let's take a look at the General Service List (list of common english words) ... 2000 words or so\n",
    "\n",
    "https://www.eapfoundation.com/vocab/general/gsl/\n",
    "\n",
    "# Corpus of Contemporary American English (COCA) <-- sorted by frequency \n",
    "https://www.wordfrequency.info/ \n",
    "UGH i have to purchase this? booo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cd6bc",
   "metadata": {},
   "source": [
    "# Game plan...\n",
    "* I want certain reactive groups to correlate with whether things are nouns or verbs.\n",
    "    * I think that means I have to somehow separate out reactive vs. unreactive chemicals first.\n",
    "    * Similarly separate out verbs and nouns in english (that should have a function already)\n",
    "* Then I want to maybe match the most common words in each category using COCA for english and some other sort of frequency of most common chemicals...\n",
    "* After frequency, let's rely on embeddings to get everything else closest to the nearest neighbor anchor point. This way we can be sure things are at least somewhat aligned.\n",
    "    * alternatively we don't even start with the anchors but jump straight into the embeddings? Need to talk to David to get his help here with alignment.\n",
    "\n",
    "\n",
    "* Subjunctive / conditional / other fun tenses could be inert gases that I add to the reaction like Helium could be a good example of conditional.\n",
    "* Electro chemistry would be crazy... maybe that can be for a tense that is crazy... again I want to say conditional.\n",
    "\n",
    "* Past, present, future will be decided by which side of a reaction is given. if it's in equilibrium then it's present.\n",
    "\n",
    "I guess stoichometry is important here :^P\n",
    "\n",
    "* One thing I wish would be that we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14358cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yitongtseo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fb37236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag descriptions based on the Penn Treebank tag set\n",
    "pos_tag_descriptions = {\n",
    "    \"CC\": \"Coordinating conjunction\",\n",
    "    \"CD\": \"Cardinal number\",\n",
    "    \"DT\": \"Determiner\",\n",
    "    \"EX\": \"Existential there\",\n",
    "    \"FW\": \"Foreign word\",\n",
    "    \"IN\": \"Preposition or subordinating conjunction\",\n",
    "    \"JJ\": \"Adjective\",\n",
    "    \"JJR\": \"Adjective\",\n",
    "    \"JJS\": \"Adjective\",\n",
    "    \"LS\": \"List item marker\",\n",
    "    \"MD\": \"Modal\",\n",
    "    \"NN\": \"Noun\",\n",
    "    \"NNS\": \"Noun\",\n",
    "    \"NNP\": \"Noun\", # Proper Noun\n",
    "    \"NNPS\": \"Noun\",\n",
    "    \"PDT\": \"Predeterminer\",\n",
    "    \"POS\": \"Possessive ending\",\n",
    "    \"PRP\": \"Noun\", # Personal pronoun\n",
    "    \"PRP$\": \"Noun\", # Possessive pronoun\n",
    "    \"RB\": \"Adverb\",\n",
    "    \"RBR\": \"Adverb\",\n",
    "    \"RBS\": \"Adverb\",\n",
    "    \"RP\": \"Particle\",\n",
    "    \"SYM\": \"Symbol\",\n",
    "    \"TO\": \"to\",\n",
    "    \"UH\": \"Interjection\",\n",
    "    \"VB\": \"Verb\",\n",
    "    \"VBD\": \"Verb\",\n",
    "    \"VBG\": \"Verb\",\n",
    "    \"VBN\": \"Verb\",\n",
    "    \"VBP\": \"Verb\",\n",
    "    \"VBZ\": \"Verb\",\n",
    "    \"WDT\": \"Wh-determiner\",\n",
    "    \"WP\": \"Wh-pronoun\",\n",
    "    \"WP$\": \"Possessive wh-pronoun\",\n",
    "    \"WRB\": \"Wh-adverb\",\n",
    "    \"``\": \"Punctuation\",\n",
    "    \"''\": \"Punctuation\",\n",
    "    \",\": \"Punctuation\",\n",
    "    \".\": \"Punctuation\",\n",
    "    \":\": \"Punctuation\",\n",
    "    \"(\": \"Punctuation\",\n",
    "    \")\": \"Punctuation\",\n",
    "    \"$\": \"Punctuation\",\n",
    "    \"#\": \"Punctuation\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "778239f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to determine the lemma based on POS tag\n",
    "def get_lemma(word, tag):\n",
    "    # Exclude words that are entirely numeric or symbols\n",
    "    if re.fullmatch(r'[\\d$.,]+', word):\n",
    "        return None  # Return None to indicate exclusion\n",
    "    \n",
    "    # Handle possessive cases\n",
    "    if word.endswith(\"'s\") or word.endswith(\"’s\"):\n",
    "        word = word[:-2]  # Remove 's\n",
    "    elif word.endswith(\"'\") or word.endswith(\"’\"):\n",
    "        word = word[:-1]  # Remove trailing apostrophe\n",
    "\n",
    "    if tag.startswith('NN'):\n",
    "        return lemmatizer.lemmatize(word, pos='n')  # for nouns\n",
    "    elif tag.startswith('VB'):\n",
    "        return lemmatizer.lemmatize(word, pos='v')  # for verbs\n",
    "    elif tag.startswith('JJ'):\n",
    "        return lemmatizer.lemmatize(word, pos='a')  # for adjectives\n",
    "    elif tag.startswith('RB'):\n",
    "        return lemmatizer.lemmatize(word, pos='r')  # for adverbs\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "\n",
    "words = [word.lower() for word in brown.words()]\n",
    "tagged_words = pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4051a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_lemmas = [\n",
    "    (get_lemma(word, pos), pos_tag_descriptions[pos]) for (word, pos) in tagged_words\n",
    "]\n",
    "total_freq_dist = FreqDist(tagged_lemmas)\n",
    "sorted_words_by_POS = {}\n",
    "for (lemma, pos), count in total_freq_dist.items():\n",
    "    sorted_words_by_POS[pos] = sorted_words_by_POS.get(pos, []) + [(lemma, count)]\n",
    "\n",
    "for pos, word_list in sorted_words_by_POS.items():\n",
    "    sorted_words_by_POS[pos] = sorted(word_list, key=lambda item: item[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a77be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24469"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_words_by_POS['Noun'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e77df5",
   "metadata": {},
   "source": [
    "### Now let's get their embeddings and roughly cluster the top 1000 or so words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_word_embeddings(words):\n",
    "    inputs = tokenizer(words, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Word:\n",
    "    pos: str\n",
    "    word: str\n",
    "    count: int\n",
    "\n",
    "    def get_embedding(self) -> float:\n",
    "        return get_word_embeddings([self.word])[0]\n",
    "\n",
    "\n",
    "# sorted_words_embeddings_by_POS = {}\n",
    "\n",
    "# for pos, word_list in sorted_words_by_POS.items():\n",
    "#     sorted_words_by_POS[pos] = sorted(word_list, key=lambda item: item[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d906755",
   "metadata": {},
   "source": [
    "# The rest is easy enough... what about getting the top molecules now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e04bff",
   "metadata": {},
   "source": [
    "Okay let's take a swing at Enamine Building Blocks, I found a list of 500K of them. that should be a good start.\n",
    "Can we create their morgan fingerprints and cluster them?\n",
    "\n",
    "Taken from https://zinc12.docking.org/db/byvendor/enaminebb/enaminebb.in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f831fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enamine_df = pd.read_csv('enaminebb.in.txt', delimiter=' ', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78efa017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 522/522 [13:58<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_single_molecule_embedding(smiles, radius=5, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "        return list(fp)\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_and_append(\n",
    "    df, start_idx=0, chunk_size=1000, output_file=\"final_processed_data.csv\"\n",
    "):\n",
    "    for start in tqdm(range(start_idx, len(df), chunk_size)):\n",
    "        end = start + chunk_size\n",
    "        chunk = df.iloc[start:end].copy()  # Using copy to safely modify the data\n",
    "        chunk[\"Morgan_fingerprint\"] = chunk[\"SMILES\"].apply(\n",
    "            get_single_molecule_embedding\n",
    "        )\n",
    "        mode = (\n",
    "            \"a\" if start > 0 else \"w\"\n",
    "        )  # 'w' for write on first chunk, 'a' for append on subsequent chunks\n",
    "        header = True if start == 0 else False  # Write header only for the first chunk\n",
    "        chunk.to_csv(output_file, mode=mode, header=header, index=False)\n",
    "\n",
    "\n",
    "def check_progress(output_file):\n",
    "    \"\"\"Check if the output file already exists and determine the last processed index\"\"\"\n",
    "    try:\n",
    "        last_part = pd.read_pickle(output_file)\n",
    "        last_index_processed = last_part.index[-1]\n",
    "        return last_index_processed + 1\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Check for existing progress and determine where to resume\n",
    "output_file = \"processed_data.pkl\"\n",
    "start_index = check_progress(output_file)\n",
    "\n",
    "# Process DataFrame from the last checkpoint\n",
    "process_and_append(\n",
    "    enamine_df, start_idx=start_index, chunk_size=1000, output_file=output_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439edc14",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "the STRING opcode argument must be quoted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m enamine_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chemistry_language/lib/python3.9/site-packages/pandas/io/pickle.py:208\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: the STRING opcode argument must be quoted"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_file = \"processed_data.pkl\"\n",
    "enamine_df = pd.read_pickle(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9d893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enamine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7588ff",
   "metadata": {},
   "source": [
    "We will use AIZynthFinder for the retrosynthesis planning\n",
    "\n",
    "https://github.com/MolecularAI/aizynthfinder\n",
    "\n",
    "https://molecularai.github.io/aizynthfinder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a9b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_model.onnx to uspto\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_templates.csv.gz to uspto\n",
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_model.onnx to ringbreaker\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_templates.csv.gz to ringbreaker\n",
      "Loading filter policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_filter_model.onnx to uspto\n",
      "Loading stock from InMemoryInchiKeyQuery to zinc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb262d1ed9494311ba93173220fb5427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', continuous_update=False, description='SMILES')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab558fd253884be3aa876bc3c3744c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid silver', height='180px', width='50%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c4b170e88545109ab8fe4287162f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Label(value='Stocks'), Checkbox(value=True, description='zinc', style=Descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0375baab1e48b6b6549633bf97f84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Run Search', style=ButtonStyle()), Button(description='Extend Search', styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df33e54c6074f90a3d5c3b22da26ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid silver', height='320px', overflow='auto', width='99%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e1446aaae14cada371620d6699be4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Show Reactions', style=ButtonStyle()), Dropdown(description='Routes: ', opt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8d0420ce0245a3a06720a5022d8d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid silver', width='99%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aizynthfinder.interfaces import AiZynthApp\n",
    "app = AiZynthApp(\"config.yml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef0a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_model.onnx to uspto\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_templates.csv.gz to uspto\n",
      "Loading template-based expansion policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_model.onnx to ringbreaker\n",
      "Loading templates from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_ringbreaker_templates.csv.gz to ringbreaker\n",
      "Loading filter policy model from /Users/yitongtseo/Documents/GitHub/ethical_necromancy/Chemical_Language(Glucose for now)/uspto_filter_model.onnx to uspto\n",
      "Loading stock from InMemoryInchiKeyQuery to zinc\n",
      "Selected as stock: zinc\n",
      "Compounds in stock: 17422831\n",
      "Selected as expansion policy: uspto\n",
      "Selected as filter policy: uspto\n",
      "Trees saved to trees.json                                                       \n",
      "Scores for best routes: 0.9976, 0.9940, 0.9940, 0.9940, 0.9940\n",
      "target: COc1cccc(OC(=O)/C=C/c2cc(OC)c(OC)c(OC)c2)c1\n",
      "search time: 110.15064024925232\n",
      "first solution time: 1.397794246673584\n",
      "first solution iteration: 1\n",
      "number of nodes: 282\n",
      "max transforms: 6\n",
      "max children: 19\n",
      "number of routes: 101\n",
      "number of solved routes: 34\n",
      "top score: 0.9976287063411217\n",
      "is solved: True\n",
      "number of steps: 1\n",
      "number of precursors: 2\n",
      "number of precursors in stock: 2\n",
      "precursors in stock: COc1cccc(OC(C)=O)c1, COc1cc(C=O)cc(OC)c1OC\n",
      "precursors not in stock: \n",
      "precursors availability: zinc;zinc\n",
      "policy used counts: {'uspto': 281}\n",
      "profiling: {'expansion_calls': 232, 'reactants_generations': 3403, 'iterations': 100}\n"
     ]
    }
   ],
   "source": [
    "!aizynthcli --config config.yml --smiles \"COc1cccc(OC(=O)/C=C/c2cc(OC)c(OC)c(OC)c2)c1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming enamine_df is your DataFrame and it has a column 'SMILES'\n",
    "# Applying the fingerprint function to each SMILES entry\n",
    "tqdm.pandas(desc=\"Generating fingerprints\")\n",
    "enamine_df[\"Morgan_fingerprint\"] = enamine_df[\"SMILES\"].progress_apply(\n",
    "    get_single_molecule_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "264a8c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 6, 4, 39, 44, 97, 38]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tqdm as tqdm\n",
    "\n",
    "# # TODO: add a column to enamine_df that is called \"Morgan_fingerprint\"\n",
    "# for idx, row in tqdm(enamine_df.iterrows(), total=len(enamine_df)):\n",
    "#     enamine_df.iloc[idx][\"Morgan Fingerprint\"] = get_molecule_embeddings(\n",
    "#         [row[\"SMILES\"]]\n",
    "#     )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # # Example usage\n",
    "# smiles_list = [\n",
    "#     \"C(C(=O)O)N\",\n",
    "#     \"CCO\",\n",
    "#     \"C1=CC=CC=C1\",\n",
    "#     \"COc1cc2ccccc2cc1C(=O)O\",\n",
    "#     \"c1ccc2c(c1)C(=O)c3cccc(c3C2=O)C(=O)O\",\n",
    "#     \"c1cc(cc(c1)Oc2cccc(c2)C(F)(F)F)C3N(C(=O)CS3)CCN4CCCCC4\",\n",
    "#     \"COC(=O)c1cccc(c1S(=O)(=O)F)N\"\n",
    "# ]  # Glycine, Ethanol, Benzene\n",
    "# fingerprints = get_molecule_embeddings(smiles_list)\n",
    "# # print(fingerprints)\n",
    "# # print(fingerprints[0].sum())\n",
    "# # print(fingerprints[1].sum())\n",
    "# [fp.sum() for fp in fingerprints]\n",
    "\n",
    "import numpy as np\n",
    "# print(np.array(fingerprints[2]).where(1))\n",
    "numpy_arr = np.array(fingerprints[2])\n",
    "[idx for idx, val  in enumerate(numpy_arr) if val > 0]\n",
    "fingerprints[2][389]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d73f40",
   "metadata": {},
   "source": [
    "## Taking a step back... nouns can be just described by their constitutive atoms? But complex nouns then get out of grasp... like how would you chemically describe a human?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open('most_common_words.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Word', 'Frequency'])  # Writing header\n",
    "    writer.writerows(most_common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f20f04",
   "metadata": {},
   "source": [
    "# PubChem to the rescue\n",
    "\n",
    "FTP to download all of the compounds in pubchem as a SMILES -> ftp://ftp.ncbi.nlm.nih.gov/pubchem/Compound/Extras/CID-SMILES.gz\n",
    "https://chemistry.stackexchange.com/questions/122109/how-to-get-the-smiles-of-all-compounds-on-pubchem \n",
    "\n",
    "Hopefully it's only a couple of gigabytes\n",
    "\n",
    "Then to get the 1000 most common molecules is a bit difficult. but perhpas something I can do is just cheat:\n",
    "* Either take the 1000 cheapest molecules\n",
    "* Or take the 1000 molecules with the shortest SMILE strings (That ensures I get all of the elements, H2O, methane, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671e0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f4a440a",
   "metadata": {},
   "source": [
    "Maybe not necessary, but still may help is to bring them both down to the same dimension size with PCA\n",
    "This step can help in normalizing the scale and variance of the datasets.\n",
    "Especially if I bring them both lower... maybe 100?\n",
    "\n",
    "* Canonical Correlation Analysis (CCA): If you seek more sophisticated statistical techniques, CCA can be used to find the relationships between two sets of variables. This might give insight`s into how molecular structures correlate with semantic meanings of words.\n",
    "* Procrustes Analysis: This method is used to find the optimal alignment of two datasets by minimizing the distance between them after allowing for translation, scaling, and rotation. It's more complex but can effectively align two spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b87d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2830541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import procrustes\n",
    "\n",
    "# Assume matrix1 and matrix2 are your datasets (numpy arrays)\n",
    "# They should be of the same dimensions\n",
    "\n",
    "# Apply Procrustes Analysis\n",
    "mtx1_transformed, mtx2_transformed, disparity = procrustes(matrix1, matrix2)\n",
    "\n",
    "# mtx1_transformed and mtx2_transformed are the transformed matrices\n",
    "# 'disparity' measures the sum of the squared differences (lower is better fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "\n",
    "# Assume X and Y are your datasets (numpy arrays) with the same number of samples\n",
    "# X could be word embeddings, and Y could be molecular fingerprints, both already preprocessed\n",
    "\n",
    "# Instantiate and fit the CCA model\n",
    "cca = CCA(n_components=2)  # You can adjust 'n_components' based on your specific needs\n",
    "cca.fit(X, Y)\n",
    "\n",
    "# Transform the datasets\n",
    "X_c, Y_c = cca.transform(X, Y)\n",
    "\n",
    "# X_c and Y_c are the projections of X and Y onto the learned canonical variables\n",
    "# You can now analyze the correlations or use these projections for further analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
